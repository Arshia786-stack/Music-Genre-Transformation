{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9cec73a6-f3ac-411f-8caa-94ca9c49d96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the genre classifier...\n",
      "Classifier Epoch 1, Loss: 2.4843\n",
      "Classifier Epoch 2, Loss: 2.3151\n",
      "Classifier Epoch 3, Loss: 2.2965\n",
      "Classifier Epoch 4, Loss: 2.3064\n",
      "Classifier Epoch 5, Loss: 2.2924\n",
      "Classifier Epoch 6, Loss: 2.2930\n",
      "Classifier Epoch 7, Loss: 1.9837\n",
      "Classifier Epoch 8, Loss: 3.2562\n",
      "Classifier Epoch 9, Loss: 2.2312\n",
      "Classifier Epoch 10, Loss: 2.1923\n",
      "Training the GAN...\n",
      "GAN Epoch [0/20] Batch [0/125] D Loss: 1.3809 G Loss: 279197.5625\n",
      "GAN Epoch [0/20] Batch [10/125] D Loss: 1.1439 G Loss: 1722914.6250\n",
      "GAN Epoch [0/20] Batch [20/125] D Loss: 1.0305 G Loss: 2214226.7500\n",
      "GAN Epoch [0/20] Batch [30/125] D Loss: 0.9264 G Loss: 128528.2812\n",
      "GAN Epoch [0/20] Batch [40/125] D Loss: 0.9133 G Loss: 916655.1875\n",
      "GAN Epoch [0/20] Batch [50/125] D Loss: 0.8592 G Loss: 503788.5312\n",
      "GAN Epoch [0/20] Batch [60/125] D Loss: 0.8334 G Loss: 206348.8906\n",
      "GAN Epoch [0/20] Batch [70/125] D Loss: 0.8021 G Loss: 1033042.3125\n",
      "GAN Epoch [0/20] Batch [80/125] D Loss: 0.7790 G Loss: 1751631.6250\n",
      "GAN Epoch [0/20] Batch [90/125] D Loss: 0.7016 G Loss: 721481.1250\n",
      "GAN Epoch [0/20] Batch [100/125] D Loss: 0.6233 G Loss: 203217.4688\n",
      "GAN Epoch [0/20] Batch [110/125] D Loss: 0.5333 G Loss: 480081.4375\n",
      "GAN Epoch [0/20] Batch [120/125] D Loss: 0.5803 G Loss: 797321.4375\n",
      "GAN Epoch [1/20] Batch [0/125] D Loss: 0.4995 G Loss: 1962016.5000\n",
      "GAN Epoch [1/20] Batch [10/125] D Loss: 0.4806 G Loss: 511019.1250\n",
      "GAN Epoch [1/20] Batch [20/125] D Loss: 0.4126 G Loss: 378447.3125\n",
      "GAN Epoch [1/20] Batch [30/125] D Loss: 0.3740 G Loss: 527148.7500\n",
      "GAN Epoch [1/20] Batch [40/125] D Loss: 0.3910 G Loss: 702243.6250\n",
      "GAN Epoch [1/20] Batch [50/125] D Loss: 0.3272 G Loss: 914120.8750\n",
      "GAN Epoch [1/20] Batch [60/125] D Loss: 0.3046 G Loss: 1103451.3750\n",
      "GAN Epoch [1/20] Batch [70/125] D Loss: 0.3285 G Loss: 60410.1953\n",
      "GAN Epoch [1/20] Batch [80/125] D Loss: 0.2881 G Loss: 666420.6875\n",
      "GAN Epoch [1/20] Batch [90/125] D Loss: 0.2766 G Loss: 250603.4219\n",
      "GAN Epoch [1/20] Batch [100/125] D Loss: 0.2506 G Loss: 987405.5000\n",
      "GAN Epoch [1/20] Batch [110/125] D Loss: 0.2372 G Loss: 278726.8750\n",
      "GAN Epoch [1/20] Batch [120/125] D Loss: 0.2439 G Loss: 211625.2031\n",
      "GAN Epoch [2/20] Batch [0/125] D Loss: 0.2278 G Loss: 351931.5312\n",
      "GAN Epoch [2/20] Batch [10/125] D Loss: 0.2351 G Loss: 136947.9844\n",
      "GAN Epoch [2/20] Batch [20/125] D Loss: 0.2105 G Loss: 453491.0312\n",
      "GAN Epoch [2/20] Batch [30/125] D Loss: 0.1976 G Loss: 802268.4375\n",
      "GAN Epoch [2/20] Batch [40/125] D Loss: 0.2160 G Loss: 369033.0625\n",
      "GAN Epoch [2/20] Batch [50/125] D Loss: 0.1782 G Loss: 916259.8125\n",
      "GAN Epoch [2/20] Batch [60/125] D Loss: 0.1778 G Loss: 153668.7500\n",
      "GAN Epoch [2/20] Batch [70/125] D Loss: 0.1718 G Loss: 211011.2500\n",
      "GAN Epoch [2/20] Batch [80/125] D Loss: 0.1725 G Loss: 531418.3750\n",
      "GAN Epoch [2/20] Batch [90/125] D Loss: 0.1685 G Loss: 1210003.6250\n",
      "GAN Epoch [2/20] Batch [100/125] D Loss: 0.1403 G Loss: 327556.5000\n",
      "GAN Epoch [2/20] Batch [110/125] D Loss: 0.1331 G Loss: 646207.9375\n",
      "GAN Epoch [2/20] Batch [120/125] D Loss: 0.1215 G Loss: 287962.9375\n",
      "GAN Epoch [3/20] Batch [0/125] D Loss: 0.1352 G Loss: 2162172.5000\n",
      "GAN Epoch [3/20] Batch [10/125] D Loss: 0.1349 G Loss: 749968.5000\n",
      "GAN Epoch [3/20] Batch [20/125] D Loss: 0.1246 G Loss: 958698.4375\n",
      "GAN Epoch [3/20] Batch [30/125] D Loss: 0.1233 G Loss: 3035287.2500\n",
      "GAN Epoch [3/20] Batch [40/125] D Loss: 0.1030 G Loss: 296444.7500\n",
      "GAN Epoch [3/20] Batch [50/125] D Loss: 0.1063 G Loss: 807048.6250\n",
      "GAN Epoch [3/20] Batch [60/125] D Loss: 0.1145 G Loss: 485621.7188\n",
      "GAN Epoch [3/20] Batch [70/125] D Loss: 0.0961 G Loss: 4241640.0000\n",
      "GAN Epoch [3/20] Batch [80/125] D Loss: 0.1165 G Loss: 233097.5938\n",
      "GAN Epoch [3/20] Batch [90/125] D Loss: 0.0919 G Loss: 876673.9375\n",
      "GAN Epoch [3/20] Batch [100/125] D Loss: 0.1094 G Loss: 217842.7031\n",
      "GAN Epoch [3/20] Batch [110/125] D Loss: 0.0962 G Loss: 2376449.7500\n",
      "GAN Epoch [3/20] Batch [120/125] D Loss: 0.0905 G Loss: 752941.1875\n",
      "GAN Epoch [4/20] Batch [0/125] D Loss: 0.1030 G Loss: 468044.2812\n",
      "GAN Epoch [4/20] Batch [10/125] D Loss: 0.0804 G Loss: 442611.0000\n",
      "GAN Epoch [4/20] Batch [20/125] D Loss: 0.0774 G Loss: 908420.8125\n",
      "GAN Epoch [4/20] Batch [30/125] D Loss: 0.0724 G Loss: 809097.1875\n",
      "GAN Epoch [4/20] Batch [40/125] D Loss: 0.0683 G Loss: 1687582.6250\n",
      "GAN Epoch [4/20] Batch [50/125] D Loss: 0.0717 G Loss: 210289.0000\n",
      "GAN Epoch [4/20] Batch [60/125] D Loss: 0.0663 G Loss: 411543.8438\n",
      "GAN Epoch [4/20] Batch [70/125] D Loss: 0.0761 G Loss: 663958.2500\n",
      "GAN Epoch [4/20] Batch [80/125] D Loss: 0.0553 G Loss: 1651073.5000\n",
      "GAN Epoch [4/20] Batch [90/125] D Loss: 0.0615 G Loss: 355310.1875\n",
      "GAN Epoch [4/20] Batch [100/125] D Loss: 0.0582 G Loss: 263908.4688\n",
      "GAN Epoch [4/20] Batch [110/125] D Loss: 0.0650 G Loss: 221142.9844\n",
      "GAN Epoch [4/20] Batch [120/125] D Loss: 0.0642 G Loss: 1930923.2500\n",
      "GAN Epoch [5/20] Batch [0/125] D Loss: 0.0583 G Loss: 927975.1250\n",
      "GAN Epoch [5/20] Batch [10/125] D Loss: 0.0527 G Loss: 191102.8594\n",
      "GAN Epoch [5/20] Batch [20/125] D Loss: 0.0576 G Loss: 147847.0781\n",
      "GAN Epoch [5/20] Batch [30/125] D Loss: 0.0650 G Loss: 399424.0625\n",
      "GAN Epoch [5/20] Batch [40/125] D Loss: 0.0557 G Loss: 304953.8438\n",
      "GAN Epoch [5/20] Batch [50/125] D Loss: 0.0508 G Loss: 56234.0508\n",
      "GAN Epoch [5/20] Batch [60/125] D Loss: 0.0448 G Loss: 739421.5625\n",
      "GAN Epoch [5/20] Batch [70/125] D Loss: 0.0464 G Loss: 675535.5625\n",
      "GAN Epoch [5/20] Batch [80/125] D Loss: 0.0405 G Loss: 828866.6875\n",
      "GAN Epoch [5/20] Batch [90/125] D Loss: 0.0490 G Loss: 142446.5625\n",
      "GAN Epoch [5/20] Batch [100/125] D Loss: 0.0529 G Loss: 661152.6875\n",
      "GAN Epoch [5/20] Batch [110/125] D Loss: 0.0417 G Loss: 598251.5000\n",
      "GAN Epoch [5/20] Batch [120/125] D Loss: 0.0483 G Loss: 291193.2188\n",
      "GAN Epoch [6/20] Batch [0/125] D Loss: 0.0394 G Loss: 350839.0312\n",
      "GAN Epoch [6/20] Batch [10/125] D Loss: 0.0430 G Loss: 1086311.1250\n",
      "GAN Epoch [6/20] Batch [20/125] D Loss: 0.0713 G Loss: 152864.2500\n",
      "GAN Epoch [6/20] Batch [30/125] D Loss: 0.0617 G Loss: 1221377.7500\n",
      "GAN Epoch [6/20] Batch [40/125] D Loss: 0.0462 G Loss: 1297622.8750\n",
      "GAN Epoch [6/20] Batch [50/125] D Loss: 0.0491 G Loss: 2835415.5000\n",
      "GAN Epoch [6/20] Batch [60/125] D Loss: 0.0394 G Loss: 153263.3750\n",
      "GAN Epoch [6/20] Batch [70/125] D Loss: 0.0360 G Loss: 1636556.0000\n",
      "GAN Epoch [6/20] Batch [80/125] D Loss: 0.0446 G Loss: 279011.0625\n",
      "GAN Epoch [6/20] Batch [90/125] D Loss: 0.0468 G Loss: 400817.8438\n",
      "GAN Epoch [6/20] Batch [100/125] D Loss: 0.0329 G Loss: 558014.3750\n",
      "GAN Epoch [6/20] Batch [110/125] D Loss: 0.0519 G Loss: 81904.3984\n",
      "GAN Epoch [6/20] Batch [120/125] D Loss: 0.1144 G Loss: 335985.6250\n",
      "GAN Epoch [7/20] Batch [0/125] D Loss: 0.0597 G Loss: 888677.8750\n",
      "GAN Epoch [7/20] Batch [10/125] D Loss: 0.1213 G Loss: 998837.8750\n",
      "GAN Epoch [7/20] Batch [20/125] D Loss: 0.0452 G Loss: 327081.6250\n",
      "GAN Epoch [7/20] Batch [30/125] D Loss: 0.0456 G Loss: 419749.9375\n",
      "GAN Epoch [7/20] Batch [40/125] D Loss: 0.0450 G Loss: 499706.4688\n",
      "GAN Epoch [7/20] Batch [50/125] D Loss: 0.0442 G Loss: 104805.6016\n",
      "GAN Epoch [7/20] Batch [60/125] D Loss: 0.0334 G Loss: 167656.6250\n",
      "GAN Epoch [7/20] Batch [70/125] D Loss: 0.0425 G Loss: 116028.9531\n",
      "GAN Epoch [7/20] Batch [80/125] D Loss: 0.0312 G Loss: 213345.3594\n",
      "GAN Epoch [7/20] Batch [90/125] D Loss: 0.0356 G Loss: 223743.3438\n",
      "GAN Epoch [7/20] Batch [100/125] D Loss: 0.0250 G Loss: 1048160.5000\n",
      "GAN Epoch [7/20] Batch [110/125] D Loss: 0.0271 G Loss: 1275657.0000\n",
      "GAN Epoch [7/20] Batch [120/125] D Loss: 0.0290 G Loss: 437246.9375\n",
      "GAN Epoch [8/20] Batch [0/125] D Loss: 0.0293 G Loss: 549826.0625\n",
      "GAN Epoch [8/20] Batch [10/125] D Loss: 0.0383 G Loss: 196401.0156\n",
      "GAN Epoch [8/20] Batch [20/125] D Loss: 0.0275 G Loss: 599930.0625\n",
      "GAN Epoch [8/20] Batch [30/125] D Loss: 0.0317 G Loss: 632297.8125\n",
      "GAN Epoch [8/20] Batch [40/125] D Loss: 0.0283 G Loss: 752240.0000\n",
      "GAN Epoch [8/20] Batch [50/125] D Loss: 0.0233 G Loss: 379120.3750\n",
      "GAN Epoch [8/20] Batch [60/125] D Loss: 0.0335 G Loss: 148909.9062\n",
      "GAN Epoch [8/20] Batch [70/125] D Loss: 0.0300 G Loss: 379996.9375\n",
      "GAN Epoch [8/20] Batch [80/125] D Loss: 0.0285 G Loss: 342676.0625\n",
      "GAN Epoch [8/20] Batch [90/125] D Loss: 0.0248 G Loss: 204830.7031\n",
      "GAN Epoch [8/20] Batch [100/125] D Loss: 0.0224 G Loss: 160255.6406\n",
      "GAN Epoch [8/20] Batch [110/125] D Loss: 0.0181 G Loss: 263898.6875\n",
      "GAN Epoch [8/20] Batch [120/125] D Loss: 0.0223 G Loss: 473370.3750\n",
      "GAN Epoch [9/20] Batch [0/125] D Loss: 0.0203 G Loss: 228035.5312\n",
      "GAN Epoch [9/20] Batch [10/125] D Loss: 0.0193 G Loss: 163704.6875\n",
      "GAN Epoch [9/20] Batch [20/125] D Loss: 0.0189 G Loss: 444082.4375\n",
      "GAN Epoch [9/20] Batch [30/125] D Loss: 0.0192 G Loss: 493266.6250\n",
      "GAN Epoch [9/20] Batch [40/125] D Loss: 0.0222 G Loss: 361213.7500\n",
      "GAN Epoch [9/20] Batch [50/125] D Loss: 0.0188 G Loss: 670169.5625\n",
      "GAN Epoch [9/20] Batch [60/125] D Loss: 0.0315 G Loss: 91501.7812\n",
      "GAN Epoch [9/20] Batch [70/125] D Loss: 0.0151 G Loss: 772780.6250\n",
      "GAN Epoch [9/20] Batch [80/125] D Loss: 0.0247 G Loss: 1566014.7500\n",
      "GAN Epoch [9/20] Batch [90/125] D Loss: 0.0196 G Loss: 611611.8750\n",
      "GAN Epoch [9/20] Batch [100/125] D Loss: 0.0903 G Loss: 50249.6289\n",
      "GAN Epoch [9/20] Batch [110/125] D Loss: 0.0263 G Loss: 141259.7969\n",
      "GAN Epoch [9/20] Batch [120/125] D Loss: 0.0373 G Loss: 1640270.6250\n",
      "GAN Epoch [10/20] Batch [0/125] D Loss: 0.0274 G Loss: 1628875.8750\n",
      "GAN Epoch [10/20] Batch [10/125] D Loss: 0.0263 G Loss: 2707120.0000\n",
      "GAN Epoch [10/20] Batch [20/125] D Loss: 0.0260 G Loss: 1833465.6250\n",
      "GAN Epoch [10/20] Batch [30/125] D Loss: 0.0181 G Loss: 165746.1719\n",
      "GAN Epoch [10/20] Batch [40/125] D Loss: 0.0240 G Loss: 1510729.0000\n",
      "GAN Epoch [10/20] Batch [50/125] D Loss: 0.0203 G Loss: 592120.8125\n",
      "GAN Epoch [10/20] Batch [60/125] D Loss: 0.0228 G Loss: 91660.0078\n",
      "GAN Epoch [10/20] Batch [70/125] D Loss: 0.0179 G Loss: 499669.7500\n",
      "GAN Epoch [10/20] Batch [80/125] D Loss: 0.0146 G Loss: 205665.4844\n",
      "GAN Epoch [10/20] Batch [90/125] D Loss: 0.0204 G Loss: 216392.6875\n",
      "GAN Epoch [10/20] Batch [100/125] D Loss: 0.0208 G Loss: 370023.0000\n",
      "GAN Epoch [10/20] Batch [110/125] D Loss: 0.0134 G Loss: 359739.5312\n",
      "GAN Epoch [10/20] Batch [120/125] D Loss: 0.0245 G Loss: 86434.2578\n",
      "GAN Epoch [11/20] Batch [0/125] D Loss: 0.0115 G Loss: 378929.8438\n",
      "GAN Epoch [11/20] Batch [10/125] D Loss: 0.0145 G Loss: 118571.8281\n",
      "GAN Epoch [11/20] Batch [20/125] D Loss: 0.0137 G Loss: 994588.6875\n",
      "GAN Epoch [11/20] Batch [30/125] D Loss: 0.0140 G Loss: 637100.8125\n",
      "GAN Epoch [11/20] Batch [40/125] D Loss: 0.0116 G Loss: 786650.7500\n",
      "GAN Epoch [11/20] Batch [50/125] D Loss: 0.0124 G Loss: 491119.3125\n",
      "GAN Epoch [11/20] Batch [60/125] D Loss: 0.0123 G Loss: 1552801.3750\n",
      "GAN Epoch [11/20] Batch [70/125] D Loss: 0.0238 G Loss: 160269.5625\n",
      "GAN Epoch [11/20] Batch [80/125] D Loss: 0.0110 G Loss: 487645.8438\n",
      "GAN Epoch [11/20] Batch [90/125] D Loss: 0.0155 G Loss: 1325583.5000\n",
      "GAN Epoch [11/20] Batch [100/125] D Loss: 0.0160 G Loss: 364442.9688\n",
      "GAN Epoch [11/20] Batch [110/125] D Loss: 0.0096 G Loss: 502830.5312\n",
      "GAN Epoch [11/20] Batch [120/125] D Loss: 0.0144 G Loss: 95748.9844\n",
      "GAN Epoch [12/20] Batch [0/125] D Loss: 0.0098 G Loss: 213575.1719\n",
      "GAN Epoch [12/20] Batch [10/125] D Loss: 0.0096 G Loss: 369230.0938\n",
      "GAN Epoch [12/20] Batch [20/125] D Loss: 0.0103 G Loss: 1462412.1250\n",
      "GAN Epoch [12/20] Batch [30/125] D Loss: 0.0103 G Loss: 1379125.3750\n",
      "GAN Epoch [12/20] Batch [40/125] D Loss: 0.0186 G Loss: 96330.0859\n",
      "GAN Epoch [12/20] Batch [50/125] D Loss: 0.0090 G Loss: 465789.2500\n",
      "GAN Epoch [12/20] Batch [60/125] D Loss: 0.0097 G Loss: 564121.1875\n",
      "GAN Epoch [12/20] Batch [70/125] D Loss: 0.0090 G Loss: 738193.5625\n",
      "GAN Epoch [12/20] Batch [80/125] D Loss: 0.0121 G Loss: 588221.8750\n",
      "GAN Epoch [12/20] Batch [90/125] D Loss: 0.0159 G Loss: 1107804.1250\n",
      "GAN Epoch [12/20] Batch [100/125] D Loss: 0.0107 G Loss: 302819.9062\n",
      "GAN Epoch [12/20] Batch [110/125] D Loss: 0.0562 G Loss: 140679.9844\n",
      "GAN Epoch [12/20] Batch [120/125] D Loss: 0.0258 G Loss: 3694116.2500\n",
      "GAN Epoch [13/20] Batch [0/125] D Loss: 0.0504 G Loss: 176581.7500\n",
      "GAN Epoch [13/20] Batch [10/125] D Loss: 0.0282 G Loss: 953469.2500\n",
      "GAN Epoch [13/20] Batch [20/125] D Loss: 0.0187 G Loss: 1030850.6875\n",
      "GAN Epoch [13/20] Batch [30/125] D Loss: 0.0192 G Loss: 217574.2812\n",
      "GAN Epoch [13/20] Batch [40/125] D Loss: 0.0144 G Loss: 1731121.2500\n",
      "GAN Epoch [13/20] Batch [50/125] D Loss: 0.0129 G Loss: 719269.6250\n",
      "GAN Epoch [13/20] Batch [60/125] D Loss: 0.0115 G Loss: 1344715.1250\n",
      "GAN Epoch [13/20] Batch [70/125] D Loss: 0.0103 G Loss: 787643.5625\n",
      "GAN Epoch [13/20] Batch [80/125] D Loss: 0.0084 G Loss: 208524.8750\n",
      "GAN Epoch [13/20] Batch [90/125] D Loss: 0.0107 G Loss: 400921.6875\n",
      "GAN Epoch [13/20] Batch [100/125] D Loss: 0.0115 G Loss: 108477.1875\n",
      "GAN Epoch [13/20] Batch [110/125] D Loss: 0.0081 G Loss: 789113.3750\n",
      "GAN Epoch [13/20] Batch [120/125] D Loss: 0.0066 G Loss: 1881107.3750\n",
      "GAN Epoch [14/20] Batch [0/125] D Loss: 0.0088 G Loss: 123544.6172\n",
      "GAN Epoch [14/20] Batch [10/125] D Loss: 0.0094 G Loss: 302134.2812\n",
      "GAN Epoch [14/20] Batch [20/125] D Loss: 0.0120 G Loss: 1094656.5000\n",
      "GAN Epoch [14/20] Batch [30/125] D Loss: 0.0069 G Loss: 356909.8750\n",
      "GAN Epoch [14/20] Batch [40/125] D Loss: 0.0086 G Loss: 921922.4375\n",
      "GAN Epoch [14/20] Batch [50/125] D Loss: 0.0079 G Loss: 317378.0312\n",
      "GAN Epoch [14/20] Batch [60/125] D Loss: 0.0071 G Loss: 922200.7500\n",
      "GAN Epoch [14/20] Batch [70/125] D Loss: 0.0077 G Loss: 490974.6875\n",
      "GAN Epoch [14/20] Batch [80/125] D Loss: 0.0073 G Loss: 500433.9375\n",
      "GAN Epoch [14/20] Batch [90/125] D Loss: 0.0120 G Loss: 653870.5000\n",
      "GAN Epoch [14/20] Batch [100/125] D Loss: 0.0067 G Loss: 1117591.3750\n",
      "GAN Epoch [14/20] Batch [110/125] D Loss: 0.0087 G Loss: 529435.5625\n",
      "GAN Epoch [14/20] Batch [120/125] D Loss: 0.0122 G Loss: 1136945.1250\n",
      "GAN Epoch [15/20] Batch [0/125] D Loss: 0.0070 G Loss: 172368.9531\n",
      "GAN Epoch [15/20] Batch [10/125] D Loss: 0.0066 G Loss: 687823.8125\n",
      "GAN Epoch [15/20] Batch [20/125] D Loss: 0.0063 G Loss: 1532308.3750\n",
      "GAN Epoch [15/20] Batch [30/125] D Loss: 0.0060 G Loss: 1373968.3750\n",
      "GAN Epoch [15/20] Batch [40/125] D Loss: 0.0061 G Loss: 1136483.6250\n",
      "GAN Epoch [15/20] Batch [50/125] D Loss: 0.0086 G Loss: 746979.8750\n",
      "GAN Epoch [15/20] Batch [60/125] D Loss: 0.0063 G Loss: 586345.1250\n",
      "GAN Epoch [15/20] Batch [70/125] D Loss: 0.0123 G Loss: 329990.7188\n",
      "GAN Epoch [15/20] Batch [80/125] D Loss: 0.0054 G Loss: 281216.0625\n",
      "GAN Epoch [15/20] Batch [90/125] D Loss: 0.0089 G Loss: 1054558.7500\n",
      "GAN Epoch [15/20] Batch [100/125] D Loss: 0.0081 G Loss: 355063.0938\n",
      "GAN Epoch [15/20] Batch [110/125] D Loss: 0.0055 G Loss: 606305.7500\n",
      "GAN Epoch [15/20] Batch [120/125] D Loss: 0.0067 G Loss: 327011.1875\n",
      "GAN Epoch [16/20] Batch [0/125] D Loss: 0.0094 G Loss: 118573.6094\n",
      "GAN Epoch [16/20] Batch [10/125] D Loss: 0.0057 G Loss: 322640.0000\n",
      "GAN Epoch [16/20] Batch [20/125] D Loss: 0.0068 G Loss: 367114.3750\n",
      "GAN Epoch [16/20] Batch [30/125] D Loss: 0.0071 G Loss: 759352.3125\n",
      "GAN Epoch [16/20] Batch [40/125] D Loss: 0.0061 G Loss: 2701337.7500\n",
      "GAN Epoch [16/20] Batch [50/125] D Loss: 0.0066 G Loss: 163536.3750\n",
      "GAN Epoch [16/20] Batch [60/125] D Loss: 0.0057 G Loss: 148363.8750\n",
      "GAN Epoch [16/20] Batch [70/125] D Loss: 0.0054 G Loss: 523798.5312\n",
      "GAN Epoch [16/20] Batch [80/125] D Loss: 0.0049 G Loss: 281445.1875\n",
      "GAN Epoch [16/20] Batch [90/125] D Loss: 0.0051 G Loss: 444980.0625\n",
      "GAN Epoch [16/20] Batch [100/125] D Loss: 0.0062 G Loss: 251525.5781\n",
      "GAN Epoch [16/20] Batch [110/125] D Loss: 0.0048 G Loss: 1152299.8750\n",
      "GAN Epoch [16/20] Batch [120/125] D Loss: 0.0045 G Loss: 813867.5625\n",
      "GAN Epoch [17/20] Batch [0/125] D Loss: 0.0061 G Loss: 721888.6250\n",
      "GAN Epoch [17/20] Batch [10/125] D Loss: 0.0049 G Loss: 1495051.6250\n",
      "GAN Epoch [17/20] Batch [20/125] D Loss: 0.0068 G Loss: 510756.6562\n",
      "GAN Epoch [17/20] Batch [30/125] D Loss: 0.0046 G Loss: 401389.7500\n",
      "GAN Epoch [17/20] Batch [40/125] D Loss: 0.0055 G Loss: 189995.5000\n",
      "GAN Epoch [17/20] Batch [50/125] D Loss: 0.0047 G Loss: 420362.2188\n",
      "GAN Epoch [17/20] Batch [60/125] D Loss: 0.0047 G Loss: 676857.6250\n",
      "GAN Epoch [17/20] Batch [70/125] D Loss: 0.0066 G Loss: 269316.4375\n",
      "GAN Epoch [17/20] Batch [80/125] D Loss: 0.0050 G Loss: 651514.8125\n",
      "GAN Epoch [17/20] Batch [90/125] D Loss: 0.0045 G Loss: 1199540.7500\n",
      "GAN Epoch [17/20] Batch [100/125] D Loss: 0.0040 G Loss: 2942780.0000\n",
      "GAN Epoch [17/20] Batch [110/125] D Loss: 0.0044 G Loss: 757193.5000\n",
      "GAN Epoch [17/20] Batch [120/125] D Loss: 0.0046 G Loss: 381295.0000\n",
      "GAN Epoch [18/20] Batch [0/125] D Loss: 0.0072 G Loss: 664823.9375\n",
      "GAN Epoch [18/20] Batch [10/125] D Loss: 0.0054 G Loss: 1818163.8750\n",
      "GAN Epoch [18/20] Batch [20/125] D Loss: 0.0050 G Loss: 324167.5625\n",
      "GAN Epoch [18/20] Batch [30/125] D Loss: 0.0048 G Loss: 2038697.5000\n",
      "GAN Epoch [18/20] Batch [40/125] D Loss: 0.0039 G Loss: 1871787.0000\n",
      "GAN Epoch [18/20] Batch [50/125] D Loss: 0.0065 G Loss: 278412.9688\n",
      "GAN Epoch [18/20] Batch [60/125] D Loss: 0.0055 G Loss: 577012.0625\n",
      "GAN Epoch [18/20] Batch [70/125] D Loss: 0.0053 G Loss: 285760.0625\n",
      "GAN Epoch [18/20] Batch [80/125] D Loss: 0.0055 G Loss: 281041.8750\n",
      "GAN Epoch [18/20] Batch [90/125] D Loss: 0.0051 G Loss: 643907.4375\n",
      "GAN Epoch [18/20] Batch [100/125] D Loss: 0.0048 G Loss: 464978.7188\n",
      "GAN Epoch [18/20] Batch [110/125] D Loss: 0.0039 G Loss: 1862416.6250\n",
      "GAN Epoch [18/20] Batch [120/125] D Loss: 0.0049 G Loss: 240792.0938\n",
      "GAN Epoch [19/20] Batch [0/125] D Loss: 0.0044 G Loss: 155417.1719\n",
      "GAN Epoch [19/20] Batch [10/125] D Loss: 0.0068 G Loss: 492376.9062\n",
      "GAN Epoch [19/20] Batch [20/125] D Loss: 0.0101 G Loss: 1524446.2500\n",
      "GAN Epoch [19/20] Batch [30/125] D Loss: 0.0057 G Loss: 2018462.2500\n",
      "GAN Epoch [19/20] Batch [40/125] D Loss: 0.0057 G Loss: 380970.3438\n",
      "GAN Epoch [19/20] Batch [50/125] D Loss: 0.0063 G Loss: 452405.4375\n",
      "GAN Epoch [19/20] Batch [60/125] D Loss: 0.0056 G Loss: 152397.7188\n",
      "GAN Epoch [19/20] Batch [70/125] D Loss: 0.0062 G Loss: 233233.9062\n",
      "GAN Epoch [19/20] Batch [80/125] D Loss: 0.0035 G Loss: 1733666.6250\n",
      "GAN Epoch [19/20] Batch [90/125] D Loss: 0.0059 G Loss: 521647.2812\n",
      "GAN Epoch [19/20] Batch [100/125] D Loss: 0.0095 G Loss: 61645.5195\n",
      "GAN Epoch [19/20] Batch [110/125] D Loss: 0.0045 G Loss: 2101293.0000\n",
      "GAN Epoch [19/20] Batch [120/125] D Loss: 0.0036 G Loss: 363904.5000\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import torchaudio.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define constants\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SAMPLE_RATE = 22050\n",
    "N_MELS = 128\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "# Load GTZAN dataset\n",
    "class GTZANDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.file_paths = glob(os.path.join(root_dir, \"genres_original\", \"*\", \"*.wav\"))\n",
    "        self.labels = [os.path.basename(os.path.dirname(fp)) for fp in self.file_paths]\n",
    "        self.label_dict = {genre: idx for idx, genre in enumerate(sorted(set(self.labels)))}\n",
    "        self.transform = transforms.MelSpectrogram(\n",
    "            sample_rate=SAMPLE_RATE, \n",
    "            n_mels=N_MELS, \n",
    "            n_fft=N_FFT, \n",
    "            hop_length=HOP_LENGTH\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        label = self.label_dict[self.labels[idx]]\n",
    "        waveform, _ = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "        waveform = torch.tensor(waveform[:SAMPLE_RATE*3])  # 3 seconds clip\n",
    "        mel_spec = self.transform(waveform)  # Shape: [1, n_mels, time]\n",
    "        return mel_spec.squeeze(0), label  # Return [n_mels, time]\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = GTZANDataset(\"gtzan_dataset\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Define Transformer-based Generator\n",
    "class TransformerGenerator(nn.Module):\n",
    "    def __init__(self, input_dim=N_MELS, num_heads=4, ff_dim=256):\n",
    "        super(TransformerGenerator, self).__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, dim_feedforward=ff_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "        self.fc = nn.Linear(input_dim, input_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.permute(2, 0, 1)  # (batch, n_mels, time) -> (time, batch, n_mels)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.fc(x)\n",
    "        x = x.permute(1, 2, 0)  # Back to (batch, n_mels, time)\n",
    "        return x\n",
    "\n",
    "# Define Discriminator\n",
    "class CNNDiscriminator(nn.Module):\n",
    "    def __init__(self, input_dim=N_MELS):\n",
    "        super(CNNDiscriminator, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # Input: [batch, 1, n_mels, time]\n",
    "            nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 2), padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dim: [batch, 1, n_mels, time]\n",
    "        return self.conv(x)\n",
    "\n",
    "# Define Genre Classifier\n",
    "class GenreClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=N_MELS, num_classes=10):\n",
    "        super(GenreClassifier, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        # Calculate the flattened size\n",
    "        self.flattened_size = 128 * (N_MELS//8) * ((3*SAMPLE_RATE//HOP_LENGTH)//8)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.flattened_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize models\n",
    "generator = TransformerGenerator().to(device)\n",
    "discriminator = CNNDiscriminator().to(device)\n",
    "classifier = GenreClassifier().to(device)\n",
    "\n",
    "# Train the classifier first\n",
    "print(\"Training the genre classifier...\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "classifier_optimizer = optim.Adam(classifier.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(10):  # Train classifier for 10 epochs\n",
    "    for mel_specs, labels in train_loader:\n",
    "        mel_specs, labels = mel_specs.to(device), labels.to(device)\n",
    "        \n",
    "        classifier_optimizer.zero_grad()\n",
    "        outputs = classifier(mel_specs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        classifier_optimizer.step()\n",
    "    \n",
    "    print(f\"Classifier Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Save the trained classifier\n",
    "torch.save(classifier.state_dict(), \"genre_classifier.pth\")\n",
    "classifier.eval()\n",
    "\n",
    "# Define losses for GAN\n",
    "adversarial_loss = nn.BCELoss()\n",
    "content_loss = nn.MSELoss()\n",
    "cycle_loss = nn.L1Loss()\n",
    "\n",
    "# Optimizers\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "\n",
    "# Training loop for GAN\n",
    "print(\"Training the GAN...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch_idx, (real_data, _) in enumerate(train_loader):\n",
    "        real_data = real_data.to(device)\n",
    "        batch_size = real_data.size(0)\n",
    "\n",
    "        # Generate fake data\n",
    "        fake_data = generator(real_data)\n",
    "\n",
    "        # Train Discriminator\n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "        d_optimizer.zero_grad()\n",
    "        \n",
    "        # Real data\n",
    "        real_output = discriminator(real_data)\n",
    "        real_loss = adversarial_loss(real_output, real_labels)\n",
    "        \n",
    "        # Fake data\n",
    "        fake_output = discriminator(fake_data.detach())\n",
    "        fake_loss = adversarial_loss(fake_output, fake_labels)\n",
    "        \n",
    "        d_loss = real_loss + fake_loss\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # Train Generator\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss_adv = adversarial_loss(discriminator(fake_data), real_labels)\n",
    "        g_loss_content = content_loss(fake_data, real_data)\n",
    "        g_loss_cycle = cycle_loss(generator(fake_data), real_data)\n",
    "        g_loss = g_loss_adv + 0.5 * g_loss_content + 0.5 * g_loss_cycle\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"GAN Epoch [{epoch}/{EPOCHS}] Batch [{batch_idx}/{len(train_loader)}] \"\n",
    "                  f\"D Loss: {d_loss.item():.4f} G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "# Genre conversion function\n",
    "def convert_genre(audio_tensor, source_genre, target_genre):\n",
    "    print(f\"Converting from {source_genre} to {target_genre}...\")\n",
    "    audio_tensor = audio_tensor.to(device)\n",
    "    converted_audio = generator(audio_tensor.unsqueeze(0)).squeeze(0)\n",
    "    \n",
    "    # Predict the genre of the converted audio\n",
    "    predicted_genre_idx = predict_genre(converted_audio)\n",
    "    predicted_genre = list(train_dataset.label_dict.keys())[predicted_genre_idx]\n",
    "    \n",
    "    print(f\"Converted audio predicted as: {predicted_genre}\")\n",
    "    return converted_audio\n",
    "\n",
    "# Genre prediction function\n",
    "def predict_genre(audio_tensor):\n",
    "    audio_tensor = audio_tensor.to(device)\n",
    "    with torch.no_grad():\n",
    "        prediction = classifier(audio_tensor.unsqueeze(0))\n",
    "    return torch.argmax(prediction, dim=1).item()\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2073cdc5-b17b-4fee-b1db-d4c6eb535fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available genres: ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
      "Loading and preprocessing gtzan_dataset/genres_original/metal/metal.00000.wav...\n",
      "Converting from metal to rock...\n",
      "Converting from metal to rock...\n",
      "Converted audio predicted as: metal\n",
      "Converting back to waveform...\n",
      "Converted audio saved to output_rock.wav\n"
     ]
    }
   ],
   "source": [
    "def load_and_preprocess_audio(file_path, duration=3):\n",
    "    \"\"\"Load and preprocess an audio file for conversion\"\"\"\n",
    "    # Load audio file\n",
    "    waveform, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "    \n",
    "    # Trim or pad to desired duration\n",
    "    if len(waveform) > SAMPLE_RATE * duration:\n",
    "        waveform = waveform[:SAMPLE_RATE * duration]\n",
    "    else:\n",
    "        padding = SAMPLE_RATE * duration - len(waveform)\n",
    "        waveform = np.pad(waveform, (0, padding), mode='constant')\n",
    "    \n",
    "    # Convert to mel-spectrogram\n",
    "    transform = transforms.MelSpectrogram(\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        n_mels=N_MELS,\n",
    "        n_fft=N_FFT,\n",
    "        hop_length=HOP_LENGTH\n",
    "    )\n",
    "    waveform_tensor = torch.tensor(waveform)\n",
    "    mel_spec = transform(waveform_tensor).squeeze(0)  # [n_mels, time]\n",
    "    \n",
    "    return mel_spec\n",
    "\n",
    "def convert_audio_file(input_path, output_path, source_genre, target_genre):\n",
    "    \"\"\"\n",
    "    Convert an audio file from source_genre to target_genre\n",
    "    and save the converted audio.\n",
    "    \n",
    "    Args:\n",
    "        input_path: Path to input audio file\n",
    "        output_path: Path to save converted audio\n",
    "        source_genre: Name of source genre (for logging)\n",
    "        target_genre: Name of target genre (for logging)\n",
    "    \"\"\"\n",
    "    # Load and preprocess audio\n",
    "    print(f\"Loading and preprocessing {input_path}...\")\n",
    "    mel_spec = load_and_preprocess_audio(input_path)\n",
    "    \n",
    "    # Convert genre - ensure we're in evaluation mode\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        print(f\"Converting from {source_genre} to {target_genre}...\")\n",
    "        converted_mel = convert_genre(mel_spec, source_genre, target_genre)\n",
    "    \n",
    "    # Convert mel-spectrogram back to audio (using Griffin-Lim)\n",
    "    print(\"Converting back to waveform...\")\n",
    "    \n",
    "    # Create inverse mel scale transform\n",
    "    inv_mel = transforms.InverseMelScale(\n",
    "        n_stft=N_FFT // 2 + 1,\n",
    "        n_mels=N_MELS,\n",
    "        sample_rate=SAMPLE_RATE\n",
    "    )\n",
    "    \n",
    "    # Create Griffin-Lim transform\n",
    "    griffin_lim = transforms.GriffinLim(\n",
    "        n_fft=N_FFT,\n",
    "        hop_length=HOP_LENGTH,\n",
    "        n_iter=32\n",
    "    )\n",
    "    \n",
    "    # Process through inverse transforms\n",
    "    # Add batch dimension if needed: [1, n_mels, time]\n",
    "    # Detach and move to CPU\n",
    "    converted_mel = converted_mel.unsqueeze(0).detach().cpu()\n",
    "    \n",
    "    # Convert mel to linear spectrogram\n",
    "    spec_estimate = inv_mel(converted_mel)\n",
    "    \n",
    "    # Reconstruct waveform\n",
    "    waveform = griffin_lim(spec_estimate)\n",
    "    \n",
    "    # Save the converted audio\n",
    "    # Ensure waveform is detached numpy array\n",
    "    torchaudio.save(output_path, waveform.detach(), SAMPLE_RATE)\n",
    "    print(f\"Converted audio saved to {output_path}\")\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Example file paths - replace with your actual files\n",
    "    input_audio = \"gtzan_dataset/genres_original/metal/metal.00000.wav\"  # Your input audio file\n",
    "    output_audio = \"output_rock.wav\"  # Where to save converted audio\n",
    "    \n",
    "    # Available genres from GTZAN dataset\n",
    "    available_genres = list(train_dataset.label_dict.keys())\n",
    "    print(\"Available genres:\", available_genres)\n",
    "    \n",
    "    # Convert jazz to rock (example)\n",
    "    convert_audio_file(\n",
    "        input_path=input_audio,\n",
    "        output_path=output_audio,\n",
    "        source_genre=\"metal\",\n",
    "        target_genre=\"rock\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc9c355-a98a-4c28-b49e-cecc8f075467",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
