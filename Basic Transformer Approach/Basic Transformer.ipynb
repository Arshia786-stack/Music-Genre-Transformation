{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb0dd933-3723-4478-8549-9ea5df7ee35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAMARTH\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class GenreTransferTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=128, embed_dim=256, num_heads=8, num_layers=6, num_genres=10):\n",
    "        super(GenreTransferTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)  # Spectrogram embedding\n",
    "        self.genre_embedding = nn.Embedding(num_genres, embed_dim)  # Genre embedding\n",
    "\n",
    "        encoder_layers = TransformerEncoderLayer(embed_dim, num_heads, dim_feedforward=512)\n",
    "        self.transformer = TransformerEncoder(encoder_layers, num_layers)\n",
    "\n",
    "        self.output_layer = nn.Linear(embed_dim, input_dim)  # Output transformation\n",
    "\n",
    "    def forward(self, x, genre_label):\n",
    "        x = x.permute(2, 0, 1)  # [Time, Batch, Features]\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Add genre embedding\n",
    "        genre_emb = self.genre_embedding(genre_label).unsqueeze(0)  # [1, Batch, Embedding]\n",
    "        x = x + genre_emb  # Apply genre transformation\n",
    "\n",
    "        x = self.transformer(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = x.permute(1, 2, 0)  # Reshape back to [Batch, Features, Time]\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "num_genres = 10  # GTZAN has 10 genres\n",
    "model = GenreTransferTransformer(num_genres=num_genres)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9a686ff-4214-45d4-bfdf-1d5abf15a76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 1809.5393\n",
      "Epoch [2/20], Loss: 1335.6341\n",
      "Epoch [3/20], Loss: 899.9405\n",
      "Epoch [4/20], Loss: 568.0085\n",
      "Epoch [5/20], Loss: 355.1287\n",
      "Epoch [6/20], Loss: 241.3066\n",
      "Epoch [7/20], Loss: 192.7127\n",
      "Epoch [8/20], Loss: 171.0039\n",
      "Epoch [9/20], Loss: 162.9325\n",
      "Epoch [10/20], Loss: 159.7325\n",
      "Epoch [11/20], Loss: 158.5963\n",
      "Epoch [12/20], Loss: 157.9837\n",
      "Epoch [13/20], Loss: 157.9271\n",
      "Epoch [14/20], Loss: 157.6981\n",
      "Epoch [15/20], Loss: 157.7058\n",
      "Epoch [16/20], Loss: 157.6502\n",
      "Epoch [17/20], Loss: 157.6650\n",
      "Epoch [18/20], Loss: 157.8492\n",
      "Epoch [19/20], Loss: 157.6281\n",
      "Epoch [20/20], Loss: 157.6652\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Minimize spectrogram reconstruction error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, labels)  # Pass genre labels\n",
    "        loss = criterion(outputs, inputs)  # Compare transformed spectrogram with input\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10f75c16-9a62-4edb-8bcc-966aa147c3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated file saved: transformed_audio.wav\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "def transform_audio(input_file, target_genre):\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and process input audio\n",
    "    input_spec = load_audio(input_file)\n",
    "    input_spec = torch.tensor(input_spec, dtype=torch.float32).unsqueeze(0).to(device)  # Add batch dim\n",
    "\n",
    "    # Convert genre name to index\n",
    "    genre_index = torch.tensor([genre_to_idx[target_genre]], dtype=torch.long).to(device)\n",
    "\n",
    "    # Transform the spectrogram\n",
    "    with torch.no_grad():\n",
    "        transformed_spec = model(input_spec, genre_index).squeeze(0).cpu().numpy()\n",
    "\n",
    "    # Convert spectrogram back to audio\n",
    "    generated_audio = librosa.feature.inverse.mel_to_audio(librosa.db_to_power(transformed_spec))\n",
    "\n",
    "    # Save output as WAV\n",
    "    output_file = \"transformed_audio.wav\"\n",
    "    sf.write(output_file, generated_audio, 22050)\n",
    "\n",
    "    print(f\"Generated file saved: {output_file}\")\n",
    "    return transformed_spec, output_file\n",
    "\n",
    "# Example Usage\n",
    "input_audio = \"gtzan_dataset/genres_original/blues/blues.00000.wav\"  # Replace with actual file\n",
    "input_audio = sample_file\n",
    "target_genre = \"jazz\"  # Example target genre\n",
    "transformed_spec, output_audio = transform_audio(input_audio, target_genre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77a4b904-ccb8-447b-8135-12735876d697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Genre: country\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def classify_audio(file_path):\n",
    "    spec = load_audio(file_path)\n",
    "    spec = torch.tensor(spec, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(spec, torch.zeros(1, dtype=torch.long).to(device))  # No genre transformation\n",
    "        predicted_genre_idx = torch.argmax(torch.mean(outputs, dim=-1), dim=-1).cpu().item()\n",
    "\n",
    "    predicted_genre = GENRES[predicted_genre_idx]\n",
    "    print(f\"Predicted Genre: {predicted_genre}\")\n",
    "    return predicted_genre\n",
    "\n",
    "# Verify transformed output\n",
    "predicted_genre = classify_audio(output_audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b53fc7a-f741-4f01-94c4-7ef9561f8844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
